#+title: Yan Zhang's CV

* Contact
- Yan Zhang, dirtysalt1987 AT gmail DOT com
- GitHub: https://github.com/dirtysalt/
- LinkedIn: https://www.linkedin.com/in/dirtysalt

* Summary
Extensive experience in:
- large-scale distributed system design and implementation.
- network programming framework design and implementation.
- storage system design and implementation.
- performance optimization and tuning for systems and applications.
- big data process and analysis.
- data mining and machine learning.

Specialties:
- proficient in C/C++, Python, Java, Scala.
- solid knowledge of data structure and algorithm.
- extremely familiar with system development on Linux.
- good understanding of compiler technique and related tools.

* Experience
** Software Engineer, [[http://castbox.fm/][CastBox.FM]], 2016.4 - (Head of Backend Development)

- crawler system. It fetches podcasts available on the internet, synchronizes with them, and notify uses once new episodes are available. The software stacks include Python, Requests, Beautifulsoup, FeedParser, Squid, MongoDB etc. By collecting RSS feeds submitted by users and search keywords, the number of podcasts in our database has been increased from 200k to 600k, and the number of episodes has been increased from 20m to 40m. By applying machine learning algorithm on the released date of episodes in the past, we can predict the future released date and increase responsiveness, new episodes will be fetched by our crawler in 5 minutes once they are released by podcasters, and users on the app will be notified at the same time. Meanwhile, we optimize images of episodes by compressing and cropping them, reduce the size of images from MB level to less than 300KB, which saves network traffic and reduces image loading time on the mobile app.

- search system. Users can search for podcasts and episodes by keywords and get keyword suggestions. It's developed on ElasticSearch and supports up to 12 languages including English, Portuguese, Spanish, German, Dutch, CJK etc. Data shows us more than 1/3 users subscription comes from the search system, so we put many efforts on improving and optimizing search system from following aspects.
  - index freshness. Once new episodes are fetched by our crawler system, index message will be put into the message queue(Redis) and triggers index system. The latency of the whole pipeline is less than 10 seconds and more than 20k episodes are reindexed per day.
  - search latency. By using cache effectively and fine-tuning Elasticsearch, we control the latency of search API under 200ms and the latency of suggestion API under 10ms.
  - search relevance. Besides document relevance score returned by Elasticsearch, we add many features including play numbers and subscription numbers in the past, in recent 1 day, and in recent 7 days etc, to get a better relevance score.

- recommender system. By analysis user subscription data and applying collaborative filtering algorithm, we can recommender users podcasts that they may like and find similar podcasts. We use LightFM python library and apply WARP algorithm on user subscription data in recent 3 months. With fine tuning of parameters and A/B Testing, we raise CTR of user recommended podcasts from 2.16% to 4.52%, and CTR of similar podcasts from 1.90% to 3.19%.

** Software Engineer, [[http://logzilla.net/][Logzilla]], 2015.4 - 2015.8 (Remote, as Consultant)

A real-time event analytical platform.

- performance tuning to support ~200k eps(event per second) ingestion.
- implement a new event storage engine to support ~1M eps(event per second).

** Software Engineer, [[http://galeracluster.com/][Galera]], 2014.4 - 2014.11 (Remote, as Consultant)

A drop-in plugin of MySQL multi-master.

Optimize cluster recovery process regarding data center outage case, and reduce recovery time from the 30s to less than 3s.

** Software Architect, Data Platform, [[https://www.umeng.com/][Umeng]], 2012.6 - 2016.4

- design Umeng internal Realtime+Batch Architecture. (aka. Lambda Architecture http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html)

- KVProxy, an asynchronous high-performance HTTP server for easily accessing various database systems such as HBase, MySQL, Riak etc. It's written in Scala and Finagle, use Google Protocol-Buffers as data exchange format and Google Guava LRUCache as the application-level cache. Since Finagle wraps an asynchronous function in a concept of 'Future' and encourages the developer to take server as a function(Your Server as a Function. http://monkey.org/~marius/funsrv.pdf), so KVproxy could be used not only as a server but also a library that could be easily embedded into other applications.

- performance tuning of MapReduce jobs and Hadoop cluster usage from perspectives of
  1. application. use HBase bulk-loading instead of writing data to HBase directly for better throughput and stability.
  2. algorithm. use HyperLogLog algorithm instead of using set to calculate cardinality for better performance and any-time-range query ability.
  3. system. turn off MapReduce speculative mode when reading data from HBase.
  4. language. use JNI instead of pure Java code to accelerate CPU computation.
  5. kernel. change kernel parameters like /proc/sys/vm/zone_reclaim_mode and /sys/kernel/mm/redhat_transparent_hugepage/enabled.

- "fast-hbase-rest", an asynchronous high-performance HTTP server written in Netty for easily accessing HBase in multiple languages by using Google Protocol-Buffers. Since HBase only provides underlying block cache, FastHBaseRest implements item cache on application level using Google Guava for better read performance. Comparing to HBase embedded HTTP server('hbase rest'), the access latency is 20% lower and transfer size is 40% lower. Meanwhile it has more capabilities like request rewriting.

- USched, an internal job scheduler system written from scratch to arrange jobs which are codependent. It defines and implements a DSL called JDL(Job Description Language) which is used to describe dependencies between jobs and properties of jobs. It runs as an HTTP server and provides a web-console to manage jobs including submissions and running status dashboard etc. Thousand MapReduce jobs are scheduled by USched each day while the latency is below 5sec.

** [[file:images/baidu-inf-com-2010q4.jpg][Senior Software Engineer]], [[https://www.baidu.com/][Baidu]], 2008.7 - 2012.6

- DStream, an in-house distributed real-time stream processing system in C++ like Twitter's Storm and Yahoo!'s S4. The alpha version of DStream with 10 nodes can process 1 million tuples per second while keeping the latency less than 100ms.

- Itachi, an open-source high-performance asynchronous network programming framework in C++.

- Comake2, an in-house build system in Python, takes advantages of some open-source build systems such as SCons, CMake, Google's GYP, Boost's Jam etc. It has been wildly used in Baidu for continuous integration.

- Infpack, an in-house data exchange format in C++, exceeds Google's Protocol-Buffers and Facebook's Thrift on the speed of serialization and deserialization about 20~30% faster while with 10~20% smaller size. Its generated code is carefully hand-tuned so implementation is very efficient.

- DDBS(distributed database system), an in-house distributed relational database system. I mainly worked on SQL parser to extend syntax for more capability and implementing a SPASS(single point automatic switch system) for its fault-tolerant feature.

- maintainer and developer of Baidu common libraries including BSL(Baidu standard library), Ullib(wraps socket io, file io, and some Linux syscalls etc.), ComDB(an embedded high-performance key-value storage system), memory allocator, character encoding, regular expression, signature and hash algorithm, URL handling, HTTP client, lock-free data structures and algorithms etc.

- Vitamin, an in-house tool to detect the potential bugs in C/C++ source code by static analyzation. It reports thousands of valuable warnings by scanning the whole of Baidu's code repository while keeping the rate of fake warnings relatively low.

- IDL compiler, an in-house compiler translates a DSL(domain specified language) to the code that supports data exchange between C/C++ struct/class and Mcpack(an in-house data pack like Google's Protocol-Buffers) with the help of Flex and Bison.


* Education
- MS. Computer Science. [[http://www.sdu.edu.cn/][Shandong University]]
- BE. Electronic Engineering. [[http://www.sdu.edu.cn/][Shandong University]]
